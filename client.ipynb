{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticating\n",
    "import requests\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/token\", data={\"username\": \"johndoe\", \"password\": \"secret\"})\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "token = result['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RemoteRunnable interface\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/llm-app-interface\", headers={\"Authorization\": f\"Bearer {token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: i need seven apples\n"
     ]
    },
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '422 Unprocessable Entity' for url 'http://localhost:8000/llm-app-interface/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422 for {\"detail\":[{\"loc\":[\"chat_history\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langserve\\client.py:157\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\httpx\\_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '422 Unprocessable Entity' for url 'http://localhost:8000/llm-app-interface/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI: Bye! Hope I was of assistance.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m ai \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m remote_chain\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: human})\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mextend([HumanMessage(content\u001b[38;5;241m=\u001b[39mhuman), AIMessage(content\u001b[38;5;241m=\u001b[39mai[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langserve\\client.py:386\u001b[0m, in \u001b[0;36mRemoteRunnable.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs not implemented yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1837\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[1;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1833\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[0;32m   1834\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1835\u001b[0m )\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 1837\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1839\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langserve\\client.py:374\u001b[0m, in \u001b[0;36mRemoteRunnable._ainvoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/invoke\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m     },\n\u001b[0;32m    373\u001b[0m )\n\u001b[1;32m--> 374\u001b[0m output, callback_events \u001b[38;5;241m=\u001b[39m \u001b[43m_decode_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lc_serializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_server_callback_events \u001b[38;5;129;01mand\u001b[39;00m callback_events:\n\u001b[0;32m    378\u001b[0m     handle_callbacks(run_manager, callback_events)\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langserve\\client.py:230\u001b[0m, in \u001b[0;36m_decode_response\u001b[1;34m(serializer, response, is_batch)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode_response\u001b[39m(\n\u001b[0;32m    224\u001b[0m     serializer: Serializer,\n\u001b[0;32m    225\u001b[0m     response: httpx\u001b[38;5;241m.\u001b[39mResponse,\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    227\u001b[0m     is_batch: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Union[List[CallbackEventDict], List[List[CallbackEventDict]]]]:\n\u001b[0;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode the response.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m     \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     obj \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wafoe\\Documents\\GitHub projects\\llm-app-interface\\.venv\\Lib\\site-packages\\langserve\\client.py:165\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m    163\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError(\n\u001b[0;32m    166\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m    167\u001b[0m     request\u001b[38;5;241m=\u001b[39m_sanitize_request(e\u001b[38;5;241m.\u001b[39mrequest),\n\u001b[0;32m    168\u001b[0m     response\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse,\n\u001b[0;32m    169\u001b[0m )\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '422 Unprocessable Entity' for url 'http://localhost:8000/llm-app-interface/invoke'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422 for {\"detail\":[{\"loc\":[\"chat_history\"],\"msg\":\"field required\",\"type\":\"value_error.missing\"}]}"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    human = input(\"Human (Q/q to quit): \")\n",
    "    print(f\"Human: {human}\")\n",
    "    if human in {\"q\", \"Q\"}:\n",
    "        print('AI: Bye! Hope I was of assistance.')\n",
    "        break\n",
    "    ai = await remote_chain.ainvoke({\"messages\": human})\n",
    "    print(f\"AI: {ai['output']}\")\n",
    "    #chat_history.extend([HumanMessage(content=human), AIMessage(content=ai['output'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the functions seem related to the command. Can you please clarify what you are looking for?\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"my dad told me shakespeare is 70 years old is that true?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grocery-list\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"id like to buy apples and oranges at the supermarket later today\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object-and-hand-recognition\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"can you help me look for apples i can't find them\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barcode-scanner\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"i want to know how many calories these cereals have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, none of the functions seem related to calling your dad. Could you please clarify what you need assistance with?\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"please call my dad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "navigation\n"
     ]
    }
   ],
   "source": [
    "print(remote_chain.invoke(\"hello hello wheres the supermarket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqb2huZG9lIiwiZXhwIjoxNzIzNDc2NjEwfQ.gsuVQrbdURf6qbvWmQYj28arefB02Z9obpc4rEn7JG4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': {'output': 'label=grocery-list'},\n",
       " 'metadata': {'run_id': '2a7ee57a-e06e-4959-9961-820af3d7c464',\n",
       "  'feedback_tokens': []}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interfacing with the model directly\n",
    "print(token)\n",
    "\n",
    "inputs = {\"input\": {\"input\": \"i require apples?\", \"chat_history\": []}}\n",
    "response = requests.post(\"http://localhost:8000/llm-app-interface/invoke\", \n",
    "    json=inputs,\n",
    "    headers={\n",
    "        'Authorization': f\"Bearer {token}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hey man how u doing\n",
      "AI: \"While I'd love to talk, my job is to assist you with the accessibility app. How can I help you today?\"\n",
      "Human: id like to know how many calories this bread has\n",
      "AI: Do you want to scan the barcode of the bread to get detailed nutritional information about it?\n",
      "Human: yeah sounds good\n",
      "AI: label=barcode-scanner\n",
      "Human: q\n",
      "AI: Bye! Hope I was of assistance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    human = input(\"Human (Q/q to quit): \")\n",
    "    print(f\"Human: {human}\")\n",
    "    if human in {\"q\", \"Q\"}:\n",
    "        print('AI: Bye! Hope I was of assistance.')\n",
    "        break\n",
    "    ai = await remote_chain.ainvoke({\"input\": human, \"chat_history\": chat_history})\n",
    "    print(f\"AI: {ai['output']}\")\n",
    "    chat_history.extend([HumanMessage(content=human), AIMessage(content=ai['output'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you help me look for the apples?\n",
      "AI: label=object-and-hand-recognition\n",
      "Human: sure\n",
      "AI: \"While I'd love to talk, my job is to assist you with the accessibility app. How can I help you today?\"\n",
      "Human: q\n",
      "AI: Bye! Hope I was of assistance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    human = input(\"Human (Q/q to quit): \")\n",
    "    print(f\"Human: {human}\")\n",
    "    if human in {\"q\", \"Q\"}:\n",
    "        print('AI: Bye! Hope I was of assistance.')\n",
    "        break\n",
    "    ai = await remote_chain.ainvoke({\"input\": human, \"chat_history\": chat_history})\n",
    "    print(f\"AI: {ai['output']}\")\n",
    "    chat_history.extend([HumanMessage(content=human), AIMessage(content=ai['output'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: i really need to buy apples in aldi today\n",
      "AI: label=grocery-list; extra=[+4]Apple\n",
      "Human: thanks for adding it to my list, now its time to go there\n",
      "AI: label=navigation;\n",
      "Human: q\n",
      "AI: Bye! Hope I was of assistance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    human = input(\"Human (Q/q to quit): \")\n",
    "    print(f\"Human: {human}\")\n",
    "    if human in {\"q\", \"Q\"}:\n",
    "        print('AI: Bye! Hope I was of assistance.')\n",
    "        break\n",
    "    ai = await remote_chain.ainvoke({\"input\": human, \"chat_history\": chat_history})\n",
    "    print(f\"AI: {ai['output']}\")\n",
    "    chat_history.extend([HumanMessage(content=human), AIMessage(content=ai['output'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$2b$12$7S1J9VKngWXJf8txEva6Z.XTy2eeX03pJF4RoV1dfK1Xj3/Cen7YS\n"
     ]
    }
   ],
   "source": [
    "# Hash passwords\n",
    "from passlib.context import CryptContext\n",
    "pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "human = input(\"Enter password to be hashed: \")\n",
    "print(pwd_context.hash(human))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
